{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Datasest from Kaggle Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fine tune the trained model in \"Image Enhancement\" notebook, we decided to do the fine tuning on a dataset from kaggle.  \n",
    "The chosen dataset was the following dataset:  \n",
    "https://www.kaggle.com/datasets/sanidhyak/human-face-emotions\n",
    "\n",
    "Despite all of the efforts done, which could be seen below, we were not able to feed pictures to the pre-trained model because it needed a special kind of input. Although we tried to use the model pre-processing method, we were not able to feed the pictures to the model again.  \n",
    "\n",
    "Finally, we moved on and we decided to use the .h5 pre-trained model directly as image enhancement method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-05 20:04:05.391303: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import keras\n",
    "import shutil\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import load_img\n",
    "from tensorflow.keras.utils import array_to_img\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Creating low resolution images from Face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"/Users/kavian/Desktop/data/high_resolution_images\"\n",
    "output_dir = \"/Users/kavian/Desktop/data/low_resolution_images\"\n",
    "scale_factor = 0.5  # Adjust this to set the desired low-resolution scale factor\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\") or filename.endswith(\".gif\") :\n",
    "        img = Image.open(os.path.join(input_dir, filename))\n",
    "        low_res_img = img.resize((int(img.width * scale_factor), int(img.height * scale_factor)), Image.LANCZOS)\n",
    "        low_res_img.save(os.path.join(output_dir, filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Split Dataset to train, validation and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we implemented chat gpt to split our dataset to test, train, and validation with hight and low resolution sub folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def create_subdirectories(root_dir, subdirs):\n",
    "    for subdir in subdirs:\n",
    "        os.makedirs(os.path.join(root_dir, subdir), exist_ok=True)\n",
    "\n",
    "def split_dataset(low_resolution_source_dir, high_resolution_source_dir, train_dir, test_dir, validation_dir, split_ratio=(0.8, 0.1, 0.1), random_seed=42):\n",
    "    # Get the list of low-resolution image filenames and sort them for consistency\n",
    "    low_resolution_file_list = os.listdir(low_resolution_source_dir)\n",
    "    low_resolution_file_list.sort()\n",
    "\n",
    "    # Get the list of high-resolution image filenames and sort them for consistency\n",
    "    high_resolution_file_list = os.listdir(high_resolution_source_dir)\n",
    "    high_resolution_file_list.sort()\n",
    "\n",
    "    # Shuffle both lists with a fixed random seed\n",
    "    random.seed(random_seed)\n",
    "    random.shuffle(low_resolution_file_list)\n",
    "    random.seed(random_seed)\n",
    "    random.shuffle(high_resolution_file_list)\n",
    "\n",
    "    # Split the file lists into training, test, and validation sets\n",
    "    train_files_low_res, test_files_low_res = train_test_split(low_resolution_file_list, test_size=split_ratio[1] + split_ratio[2], random_state=random_seed)\n",
    "    test_files_low_res, validation_files_low_res = train_test_split(test_files_low_res, test_size=split_ratio[2] / (split_ratio[1] + split_ratio[2]), random_state=random_seed)\n",
    "\n",
    "    # Create the directories for train, test, and validation\n",
    "    create_subdirectories(train_dir, [\"high_resolution\", \"low_resolution\"])\n",
    "    create_subdirectories(test_dir, [\"high_resolution\", \"low_resolution\"])\n",
    "    create_subdirectories(validation_dir, [\"high_resolution\", \"low_resolution\"])\n",
    "\n",
    "    # Copy low-resolution images to their respective directories\n",
    "    for filename in train_files_low_res:\n",
    "        shutil.copy(os.path.join(low_resolution_source_dir, filename), os.path.join(train_dir, \"low_resolution\", filename))\n",
    "    for filename in test_files_low_res:\n",
    "        shutil.copy(os.path.join(low_resolution_source_dir, filename), os.path.join(test_dir, \"low_resolution\", filename))\n",
    "    for filename in validation_files_low_res:\n",
    "        shutil.copy(os.path.join(low_resolution_source_dir, filename), os.path.join(validation_dir, \"low_resolution\", filename))\n",
    "\n",
    "    # Split the file lists for high-resolution images using the same random seed\n",
    "    train_files_high_res = [filename for filename in high_resolution_file_list if filename in train_files_low_res]\n",
    "    test_files_high_res = [filename for filename in high_resolution_file_list if filename in test_files_low_res]\n",
    "    validation_files_high_res = [filename for filename in high_resolution_file_list if filename in validation_files_low_res]\n",
    "\n",
    "    # Copy high-resolution images to their respective directories\n",
    "    for filename in train_files_high_res:\n",
    "        shutil.copy(os.path.join(high_resolution_source_dir, filename), os.path.join(train_dir, \"high_resolution\", filename))\n",
    "    for filename in test_files_high_res:\n",
    "        shutil.copy(os.path.join(high_resolution_source_dir, filename), os.path.join(test_dir, \"high_resolution\", filename))\n",
    "    for filename in validation_files_high_res:\n",
    "        shutil.copy(os.path.join(high_resolution_source_dir, filename), os.path.join(validation_dir, \"high_resolution\", filename))\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "random_seed = 42\n",
    "\n",
    "low_resolution_source_dir = \"/Users/kavian/Desktop/data/low_resolution_images\"\n",
    "high_resolution_source_dir = \"/Users/kavian/Desktop/data/high_resolution_images\"\n",
    "train_dir = \"/Users/kavian/Desktop/data/train\"\n",
    "test_dir = \"/Users/kavian/Desktop/data/test\"\n",
    "validation_dir = \"/Users/kavian/Desktop/data/validation\"\n",
    "split_dataset(low_resolution_source_dir, high_resolution_source_dir, train_dir, test_dir, validation_dir, random_seed=random_seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1- Importing pre-trained model from \"Image Enhancement\" notebook training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('/Users/kavian/Desktop/GBC/Second Semester/6- DL II Math/Final Project/DLIIMathProject/Notebooks/Image Enhancement/Image_Enhancement_before_finetuning.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers except the last few layers\n",
    "#for layer in pretrained_model.layers[:-2]:\n",
    "#    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, None, 1)]   0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, None, None, 64)    1664      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, None, None, 64)    36928     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, None, None, 32)    18464     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, None, None, 9)     2601      \n",
      "                                                                 \n",
      " tf.nn.depth_to_space (TFOp  (None, None, None, 1)     0         \n",
      " Lambda)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 59657 (233.04 KB)\n",
      "Trainable params: 59657 (233.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2- Preparing dataset to feed the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 346 images belonging to 2 classes.\n",
      "Found 44 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def preprocess_input(image):\n",
    "    target_size = (256, 256)\n",
    "    image = tf.image.rgb_to_grayscale(image)\n",
    "    image = tf.image.resize(image, target_size)\n",
    "    image = image / 255.0  \n",
    "    return image\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "train_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "validation_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator = train_data_generator.flow_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='input',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = validation_data_generator.flow_from_directory(\n",
    "    validation_dir,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='input',\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node 'model/conv2d/Relu' defined at (most recent call last):\n    File \"<frozen runpy>\", line 198, in _run_module_as_main\n    File \"<frozen runpy>\", line 88, in _run_code\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 604, in run_forever\n      self._run_once()\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/qy/q7v66x7544q5l2_cnqm0_8y00000gn/T/ipykernel_6556/1510933377.py\", line 2, in <module>\n      model.fit(\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1742, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1338, in train_function\n      return step_function(self, iterator)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1322, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1303, in run_step\n      outputs = model.train_step(data)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1080, in train_step\n      y_pred = self(x, training=True)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/engine/training.py\", line 569, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/engine/base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/engine/base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py\", line 321, in call\n      return self.activation(outputs)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/activations.py\", line 321, in relu\n      return backend.relu(\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/backend.py\", line 5397, in relu\n      x = tf.nn.relu(x)\nNode: 'model/conv2d/Relu'\nFused conv implementation does not support grouped convolutions for now.\n\t [[{{node model/conv2d/Relu}}]] [Op:__inference_train_function_1425]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Train the model on the new dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      3\u001b[0m     train_generator,\n\u001b[1;32m      4\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49mtrain_generator\u001b[39m.\u001b[39;49msamples \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m batch_size,\n\u001b[1;32m      5\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_generator,\n\u001b[1;32m      6\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_generator\u001b[39m.\u001b[39;49msamples \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m batch_size,\n\u001b[1;32m      7\u001b[0m     epochs\u001b[39m=\u001b[39;49mnum_epochs\n\u001b[1;32m      8\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'model/conv2d/Relu' defined at (most recent call last):\n    File \"<frozen runpy>\", line 198, in _run_module_as_main\n    File \"<frozen runpy>\", line 88, in _run_code\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 604, in run_forever\n      self._run_once()\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/qy/q7v66x7544q5l2_cnqm0_8y00000gn/T/ipykernel_6556/1510933377.py\", line 2, in <module>\n      model.fit(\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1742, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1338, in train_function\n      return step_function(self, iterator)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1322, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1303, in run_step\n      outputs = model.train_step(data)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1080, in train_step\n      y_pred = self(x, training=True)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/engine/training.py\", line 569, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/engine/base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/engine/base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py\", line 321, in call\n      return self.activation(outputs)\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/activations.py\", line 321, in relu\n      return backend.relu(\n    File \"/Users/kavian/Desktop/venv/venv/tensorflow_cpu/lib/python3.11/site-packages/keras/src/backend.py\", line 5397, in relu\n      x = tf.nn.relu(x)\nNode: 'model/conv2d/Relu'\nFused conv implementation does not support grouped convolutions for now.\n\t [[{{node model/conv2d/Relu}}]] [Op:__inference_train_function_1425]"
     ]
    }
   ],
   "source": [
    "# Train the model on the new dataset\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size,\n",
    "    epochs=num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encountered this error over and over. It seems that it goes back to the number of channels for the feeding image to the model. It should be grayscale.  \n",
    "We searched for this problem and found this:  \n",
    "https://stackoverflow.com/questions/61796021/unimplementederror-fused-conv-implementation-does-not-support-grouped-convoluti  \n",
    "and  \n",
    "https://stackoverflow.com/questions/73130599/tensorflow-fused-conv-implementation-does-not-support-grouped-convolutions  \n",
    "\n",
    "But they didn't solve our problem.  \n",
    "  \n",
    "We will work on that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
